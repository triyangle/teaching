\Question{MLE, MAP, and Lasso}
Assume a set of points $x_1, \ldots, x_n \ \in \mathbb R^d$, an unknown parameter vector $\theta^* \in \mathbb R^d$, and observations $y_1, \ldots, y_n \in \mathbb R$ generated by
\begin{align*}
	y_i = x_i ^\top \theta^* + \epsilon_i
\end{align*}
where $\epsilon_i \sim \mathcal N(0, \sigma^2)$ for some arbitrary $\sigma^2$. Note that this can equivalently be written as
\begin{align*}
y_i \sim \mathcal N (x_i ^\top \theta^*, \sigma^2)
\end{align*}
\begin{Parts}
\Part Show that performing maximum likelihood estimation under these modeling assumptions is equivalent to solving the unconstrained least squares problem. That is, show that you can formulate the optimization problem as 
\begin{equation}
\hat \theta = \arg \min_\theta \alpha \| X\theta - Y\|^2_2
\end{equation}\label{eq:unreg} 
for $\alpha > 0$, $X \in \mathbb R^{n \times d}$, $Y \in \mathbb R^n$.

\begin{solution}
\begin{align*}
	\hat \theta & = \arg\max_\theta p(y_1, \ldots, y_n | \theta) \\
	& = \arg\max_\theta \prod_{i = 1}^n p(y_i | \theta) \\
	& = \arg\max_\theta \prod_i \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( - \frac{\left( x_i ^\top \theta - y_i \right)^2}{2\sigma^2}\right) \\
	& = \arg\min_\theta \sum_i \frac{1}{2\sigma^2} \left( x_i^\top \theta - y_i\right)^2 \\
	& = \arg\min_\theta \| X\theta - Y\|_2^2
\end{align*}
\end{solution}

\Part  Now assume that $\theta^*_i$ is drawn from a distribution with probability density function $p(\theta_i^*) \propto e^{-|\theta_i^*|/t}$ where $t > 0$ is a constant. Show that performing maximum a posteriori estimation is equivalent to solving the $l$-1 regularized least squares problem. That is, show that you can formulate the optimization problem as
\begin{equation}\label{eq:lasso}
 \hat \theta = \arg\min_\theta \alpha \| X\theta - Y\|^2_2 + \beta \|\theta\|_1
\end{equation}
for $\alpha > 0$, $\beta > 0$, $X \in \mathbb R^{n \times d}$, $Y \in \mathbb R^n$.

\begin{solution}
\begin{align*}
	\hat \theta & = \arg\max_\theta p(\theta | y_1, \ldots, y_n) \\
	& = \arg\max_\theta p(y_1, \ldots, y_n | \theta) p(\theta) \\
	& = \arg\max_\theta p(\theta) \prod_{i = 1}^n p(y_i | \theta) \\
	& = \arg\min_\theta  \sum_{j = 1}^d \frac{|\theta_i|}{t} + \sum_i \frac{1}{2\sigma^2} \left(x_i^\top \theta - y_i\right)^2 \\
	& = \arg\min_\theta \frac{1}{2\sigma^2}\| X\theta - Y\|_2^2 + \frac{1}{t}\|\theta\|_1 \\
\end{align*}
\end{solution}

\Part  Consider the following $l$-2 regularized regression problem:
\begin{equation}\label{eq:ridge}
 \hat \theta = \arg\min_\theta  \| X\theta - Y\|^2_2 + \lambda \|\theta\|_2^2
\end{equation}Solve for $\hat \theta$ and show that it is a biased estimator.

\begin{solution}
\begin{align*}
\mathbb E [\hat \theta] & = \mathbb E \left[ \left(X^\top X + \lambda I_d \right)^{-1} X^\top Y  \right] \\
		& = \mathbb E \left[ \left(X^\top X + \lambda I_d \right)^{-1} X^\top (X \theta^* + \epsilon) \right] \\
		& = \left(X^\top X + \lambda I_d \right)^{-1} X^\top X \theta^* \\
		& \neq \theta^*
\end{align*}
\end{solution}

\Part Consider the optimization problem below that combines $l$-1 and $l$-2 regularization with $\gamma \in [0, 1]$:
\begin{equation}\label{eq:elastic}
 \hat \theta = \arg\min_\theta  \| X\theta - Y\|^2_2 +  \lambda \left[ \gamma \|\theta\|_2^2  + (1 - \gamma)\|\theta\|_1 \right]
\end{equation}
Show that it can be rewritten as an $l$-1 regularized problem with augmented versions of $X$ and $Y$.

\textit{Hint}: You can modify $X$ to be a specific block matrix.

\begin{solution}
Define augmented versions of $X$ and $Y$ as \begin{align*}
\tilde X & = \begin{bmatrix}
X \\
\delta I_d
\end{bmatrix} \\
\tilde Y & = \begin{bmatrix}
Y \\
\textbf{0}
\end{bmatrix}
\end{align*}
where \textbf{0} is a length $d$ vector of zeros. This implies the following
\begin{align*}
\|\tilde X\theta - \tilde Y\|_2^2 & = \left\| \begin{bmatrix} X\theta - Y \\ \delta \theta \end{bmatrix}\right\|_2^2 \\
& = \| X\theta - Y \|_2^2 + \delta^2\| \theta\|^2_2
\end{align*}
Adding on $\kappa \|\theta\|_1$ would result in the elastic-net regularization. Therefore, $\kappa$ and $\delta$ should be chosen as $\kappa = \lambda(1 - \gamma)$ and $\delta = \sqrt{\lambda \gamma}$. Then the appropriate form can be obtained:
\begin{align*}
\|\tilde X \theta + \tilde Y\|^2_2 - \lambda(1 - \gamma) \| \theta\|_1 & = \| X\theta - Y \|_2^2 + \lambda \gamma \| \theta\|^2_2 + \lambda (1 - \gamma) \|\theta\|_1
\end{align*}
\end{solution}
\end{Parts}

\newpage